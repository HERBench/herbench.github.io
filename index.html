<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="HERBench: A benchmark for multi-evidence integration in VideoQA.">
  <title>HERBench: Multi-Evidence VideoQA Benchmark</title>
  <link rel="icon" href="assets/images/favicon.svg" type="image/svg+xml">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/css/style.css?v=1.27">
</head>

<body>
  <div class="page-container">
    <!-- Title Section -->
    <header class="paper-header" id="top">

      <h1 class="paper-title">HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</h1>
      <div class="authors">
        <span class="author">Dan Ben-Ami<sup>1,*</sup>, Gabriele Serussi<sup>1,*</sup>, Kobi Cohen<sup>2</sup>, Chaim
          Baskin<sup>1</sup></span>
      </div>
      <div class="affiliations">
        <span class="affiliation"><sup>1</sup>INSIGHT Lab, Ben-Gurion University of the Negev, Israel</span>
        <span class="affiliation"><sup>2</sup>Ben-Gurion University of the Negev, Israel</span>
      </div>
      <div class="contribution-note">
        <span><sup>*</sup> Equal contribution</span>
      </div>
      <p class="equal-contrib">VideoQA benchmark where every question requires ≥3 dispersed evidence segments</p>
      <div class="paper-links">
        <a href="https://github.com/DanBenAmi/HERBench/blob/main/USAGE_GUIDE.md" class="paper-btn paper-btn-primary"
          target="_blank" rel="noopener">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 6.35 6.35">
            <g id="layer1" style="display:inline">
              <path
                d="M 4.8168655,0.00127766 C 4.6890373,0.00562477 4.5613573,0.02597387 4.4370455,0.06070573 3.9398008,0.19964558 3.4688398,0.53025887 3.0211092,0.92421897 2.1256507,1.712159 1.3105118,2.7893392 0.8806723,3.2191694 a 0.26460945,0.26460945 0 0 0 0,0.374654 L 2.7849474,5.4981 a 0.26460945,0.26460945 0 0 0 0.3731049,0 C 3.590482,5.06566 4.6594992,4.2400777 5.4374986,3.3344079 5.8265128,2.8815777 6.1505428,2.4049009 6.2849934,1.9045209 6.4194441,1.4041409 6.3263981,0.85817657 5.9041389,0.4358767 5.5874195,0.1191588 5.2003552,-0.01176471 4.8168655,0.00127766 Z M 4.4845858,1.0885499 c 0.2062348,3.439e-4 0.4126019,0.07877 0.5684415,0.234611 0.3116818,0.3116808 0.3116791,0.8236527 0,1.1353314 -0.3116845,0.3116855 -0.8251984,0.3127142 -1.1368829,0.00103 -0.3116792,-0.3116813 -0.3116845,-0.8262295 0,-1.1379151 C 4.0719866,1.1657698 4.2783537,1.0882027 4.4845858,1.0885483 Z M 2.1395096,2.6026696 3.7699012,4.2330609 C 3.6323708,4.3548619 3.4989362,4.4745025 3.3735422,4.5849774 L 1.7901776,3.0036787 C 1.9002601,2.8770351 2.0184998,2.7418139 2.1395096,2.6026696 Z"
                id="path660" style="fill:currentColor" />
              <path
                d="M 1.562284,1.1097372 A 0.26460945,0.26460945 0 0 0 1.3824494,1.1857017 L 0.1034588,2.4667605 a 0.26460945,0.26460945 0 0 0 0.1658805,0.451652 l 0.5307171,0.041858 C 1.2014267,2.5085594 1.746389,1.8496148 2.3720545,1.2156739 L 1.6051756,1.1112876 a 0.26460945,0.26460945 0 0 0 -0.042892,-0.00156 z M 1.9426226,2.5773481 C 1.8103071,2.7294949 1.6817937,2.876367 1.5607335,3.0155643 A 0.26460945,0.26460945 0 0 0 1.7359168,2.9220299 L 1.9875806,2.6223064 Z m 3.2142748,1.407666 C 4.5262947,4.6218117 3.8676357,5.1766596 3.4154019,5.5828493 l 0.041341,0.5229656 A 0.26460945,0.26460945 0 0 0 3.9078788,6.2716962 L 5.1894517,4.9921878 A 0.26460945,0.26460945 0 0 0 5.2633498,4.7699791 Z M 3.7523303,4.3870566 3.4531236,4.6387209 A 0.26460945,0.26460945 0 0 0 3.3595907,4.8201052 C 3.4985102,4.6975791 3.6454942,4.5664428 3.7972909,4.4320152 Z"
                id="path666" style="fill:currentColor" />
              <path
                d="m 1.3239925,4.7619079 a 0.2645835,0.2645835 0 0 0 -0.17968,0.0762 l -1.0586,1.05859 a 0.2645835,0.2645835 0 0 0 0,0.375 0.2645835,0.2645835 0 0 0 0.375,0 l 1.0586,-1.05859 a 0.2645835,0.2645835 0 0 0 0,-0.375 0.2645835,0.2645835 0 0 0 -0.19532,-0.0762 z"
                id="path672" style="fill:currentColor" />
              <path
                d="m 0.7947025,3.9669879 a 0.2645835,0.2645835 0 0 0 -0.17969,0.0781 l -0.5293,0.52929 a 0.2645835,0.2645835 0 0 0 0,0.375 0.2645835,0.2645835 0 0 0 0.375,0 l 0.5293,-0.52929 a 0.2645835,0.2645835 0 0 0 0,-0.375 0.2645835,0.2645835 0 0 0 -0.19531,-0.0781 z"
                id="path674" style="fill:currentColor" />
              <path
                d="m 2.1169625,5.2912079 a 0.2645835,0.2645835 0 0 0 -0.17773,0.0762 l -0.5293,0.5293 a 0.2645835,0.2645835 0 0 0 0,0.375 0.2645835,0.2645835 0 0 0 0.37305,0 l 0.52929,-0.5293 a 0.2645835,0.2645835 0 0 0 0,-0.375 0.2645835,0.2645835 0 0 0 -0.19531,-0.0762 z"
                id="path676" style="fill:currentColor" />
            </g>
          </svg>
          Get Started
        </a>
        <a href="https://huggingface.co/datasets/DanBenAmi/HERBench" class="paper-btn paper-btn-primary" target="_blank"
          rel="noopener">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
            <path
              d="M4.318 2.687C5.234 2.271 6.536 2 8 2s2.766.27 3.682.687C12.644 3.125 13 3.627 13 4c0 .374-.356.875-1.318 1.313C10.766 5.729 9.464 6 8 6s-2.766-.271-3.682-.687C3.356 4.875 3 4.373 3 4c0-.373.356-.875 1.318-1.313ZM13 5.698V7c0 .374-.356.875-1.318 1.313C10.766 8.729 9.464 9 8 9s-2.766-.271-3.682-.687C3.356 7.875 3 7.373 3 7V5.698c.271.202.58.378.904.525C4.978 6.711 6.427 7 8 7s3.022-.289 4.096-.777A4.92 4.92 0 0 0 13 5.698ZM14 4c0-1.007-.875-1.755-1.904-2.223C11.022 1.289 9.573 1 8 1s-3.022.289-4.096.777C2.875 2.245 2 2.993 2 4v9c0 1.007.875 1.755 1.904 2.223C4.978 15.71 6.427 16 8 16s3.022-.289 4.096-.777C13.125 14.755 14 14.007 14 13V4Zm-1 4.698V10c0 .374-.356.875-1.318 1.313C10.766 11.729 9.464 12 8 12s-2.766-.271-3.682-.687C3.356 10.875 3 10.373 3 10V8.698c.271.202.58.378.904.525C4.978 9.71 6.427 10 8 10s3.022-.289 4.096-.777A4.92 4.92 0 0 0 13 8.698Zm0 3V13c0 .374-.356.875-1.318 1.313C10.766 14.729 9.464 15 8 15s-2.766-.271-3.682-.687C3.356 13.875 3 13.373 3 13v-1.302c.271.202.58.378.904.525C4.978 12.71 6.427 13 8 13s3.022-.289 4.096-.777A4.92 4.92 0 0 0 13 11.698Z" />
          </svg>
          Dataset
        </a>
        <a href="https://github.com/DanBenAmi/HERBench" class="paper-btn paper-btn-primary" target="_blank"
          rel="noopener">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
            <path
              d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
          </svg>
          Code
        </a>
        <a href="https://arxiv.org/abs/2512.14870" class="paper-btn paper-btn-primary" target="_blank" rel="noopener">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
            <path
              d="M14 14V4.5L9.5 0H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2zM9.5 3A1.5 1.5 0 0 0 11 4.5h2V14a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1h5.5v2z" />
          </svg>
          arXiv
        </a>
      </div>
    </header>

    <!-- Teaser Section -->
    <section class="teaser-section">
      <div class="teaser-video-container">
        <video class="teaser-video" autoplay muted loop playsinline controls>
          <source src="assets/images/video_teaser.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </section>

    <!-- Abstract -->
    <section class="abstract-section" id="abstract">
      <h2>Abstract</h2>
      <p class="abstract-text">
        Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering
        (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning
        that
        must aggregate multiple, temporally separated visual evidence. In this direction, we present
        <strong>HERBench</strong>, a VideoQA
        benchmark purpose-built to assess multi-evidence integration across time. Each question is constructed to
        require
        aggregating at least three non-overlapping evidential cues across distinct video segments (so neither language
        priors
        nor a single snapshot can suffice). HERBench comprises 26K five-way multiple-choice questions
        organized into
        twelve
        compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence
        verification,
        and counting. To make evidential demand measurable, we introduce the <em>Minimum Required Frame-Set</em>
        (MRFS)-the
        smallest number of frames a model must fuse to answer correctly-and show that HERBench imposes
        substantially
        higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on
        HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20%
        random-guess baseline.
        We disentangle this failure into two critical bottlenecks: (1) a <strong>retrieval deficit</strong>, where frame
        selectors
        overlook key
        evidence, and (2) a <strong>fusion deficit</strong>, where models fail to integrate information even when all
        necessary evidence
        is
        provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes
        a principled
        target
        for advancing robust, compositional video understanding.
      </p>
    </section>

    <!-- Benchmark Overview Figure -->
    <section class="method-section" id="overview">
      <h2>Benchmark Overview</h2>
      <div class="method-figure">
        <img src="assets/images/Teaser_plot.png" alt="HERBench Overview" class="method-img">
      </div>
      <div class="method-description">
        <p>
          HERBench enforces high evidential requirements by design: questions draw on dispersed cues across long-form
          videos, and answers are balanced to prevent positional bias. The MRFS (Minimum Required Frame Set) metric
          reports the smallest number of frames a model with a fixed selector must fuse to answer correctly,
          separating genuine multi-frame reasoning from single-cue shortcuts.
        </p>
      </div>
    </section>

    <!-- Leaderboard -->
    <section class="results-section" id="leaderboard">
      <h2>Leaderboard</h2>
      <p class="section-description">Top-1 accuracy (%) with a 16-frame uniform budget.</p>
      <div class="table-wrapper">
        <table>
          <thead>
            <tr>
              <th>Rank</th>
              <th>Model</th>
              <th>Selector</th>
              <th>Frames</th>
              <th>Overall</th>
              <th>TR&amp;C</th>
              <th>R&amp;T</th>
              <th>GC&amp;V</th>
              <th>ME&amp;N</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>Ovis-2.5-9B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">42.1</td>
              <td style="text-align:left">18.9</td>
              <td style="text-align:left">73.5</td>
              <td style="text-align:left">46.8</td>
              <td style="text-align:left">29.2</td>
            </tr>
            <tr>
              <td>2</td>
              <td>InternVL3.5-14B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">41.5</td>
              <td style="text-align:left">37.7</td>
              <td style="text-align:left">69.3</td>
              <td style="text-align:left">31.1</td>
              <td style="text-align:left">27.8</td>
            </tr>
            <tr>
              <td>3</td>
              <td>InternVL3.5-8B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">41.1</td>
              <td style="text-align:left">33.6</td>
              <td style="text-align:left">70.2</td>
              <td style="text-align:left">29.7</td>
              <td style="text-align:left">30.8</td>
            </tr>
            <tr>
              <td>4</td>
              <td>Gemini-2.5-Flash</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">40.3</td>
              <td style="text-align:left">29.7</td>
              <td style="text-align:left">69.9</td>
              <td style="text-align:left">34.9</td>
              <td style="text-align:left">26.8</td>
            </tr>
            <tr>
              <td>5</td>
              <td>MiniCPM-V4.5-8B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">39.9</td>
              <td style="text-align:left">23.8</td>
              <td style="text-align:left">71.1</td>
              <td style="text-align:left">39.7</td>
              <td style="text-align:left">24.9</td>
            </tr>

            <tr class="hidden-row">
              <td>6</td>
              <td>Qwen2.5-VL-72B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">39.7</td>
              <td style="text-align:left">26.9</td>
              <td style="text-align:left">70.9</td>
              <td style="text-align:left">36.6</td>
              <td style="text-align:left">24.4</td>
            </tr>
            <tr class="hidden-row">
              <td>7</td>
              <td>GPT-4.1</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">39.4</td>
              <td style="text-align:left">25.4</td>
              <td style="text-align:left">66.0</td>
              <td style="text-align:left">37.1</td>
              <td style="text-align:left">29.0</td>
            </tr>
            <tr class="hidden-row">
              <td>8</td>
              <td>Qwen3-VL-8B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">38.3</td>
              <td style="text-align:left">19.0</td>
              <td style="text-align:left">68.7</td>
              <td style="text-align:left">40.6</td>
              <td style="text-align:left">25.2</td>
            </tr>
            <tr class="hidden-row">
              <td>9</td>
              <td>LLaVA-OneVision1.5-8B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">38.1</td>
              <td style="text-align:left">26.1</td>
              <td style="text-align:left">67.7</td>
              <td style="text-align:left">33.6</td>
              <td style="text-align:left">24.9</td>
            </tr>
            <tr class="hidden-row">
              <td>10</td>
              <td>Qwen2.5-VL-7B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">35.9</td>
              <td style="text-align:left">21.8</td>
              <td style="text-align:left">60.6</td>
              <td style="text-align:left">38.7</td>
              <td style="text-align:left">22.6</td>
            </tr>
            <tr class="hidden-row">
              <td>11</td>
              <td>LLaVA-OneVision-7B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">35.6</td>
              <td style="text-align:left">27.3</td>
              <td style="text-align:left">59.1</td>
              <td style="text-align:left">30.1</td>
              <td style="text-align:left">26.0</td>
            </tr>
            <tr class="hidden-row">
              <td>12</td>
              <td>Gemma-3-27B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">33.8</td>
              <td style="text-align:left">32.0</td>
              <td style="text-align:left">58.4</td>
              <td style="text-align:left">21.5</td>
              <td style="text-align:left">23.5</td>
            </tr>
            <tr class="hidden-row">
              <td>13</td>
              <td>LLaMA-4-Scout-17B</td>
              <td>Uniform</td>
              <td>16</td>
              <td style="text-align:left">31.4</td>
              <td style="text-align:left">18.8</td>
              <td style="text-align:left">57.3</td>
              <td style="text-align:left">25.5</td>
              <td style="text-align:left">24.2</td>
            </tr>

            <tr>
              <td colspan="9" class="baseline-row">Random baseline (5-way MCQ): 20%</td>
            </tr>
          </tbody>
        </table>
        <button id="toggle-leaderboard" class="show-more-btn">Show More</button>
      </div>
      <div class="highlight-card">
        <h3>Observed bottlenecks</h3>
        <p><strong>Evidence retrieval:</strong> learned frame selectors beat uniform sampling yet trail oracle evidence
          frames.<br>
          <strong>Evidence fusion:</strong> even with oracle frames, models often over-weight a single frame rather than
          integrating dispersed cues, missing the right answer.
        </p>
      </div>
    </section>



    <!-- Task Families -->
    <section class="results-section" id="tasks">
      <h2>Task Families</h2>
      <p class="section-description">
        12 compositional tasks grouped into 4 reasoning families:<br>
        1. Temporal Reasoning & Chronology (TR&C) <br>
        2. Referring & Tracking (R&T)<br>
        3. Global Consistency & Verification (GC&V)<br>
        4. Multi-Entity Aggregation & Numeracy (MEA&N).
      </p>

      <div class="task-explorer">
        <!-- Sidebar Menu -->
        <div class="task-menu" id="task-menu">

          <div class="task-family-group">
            <h4>Temporal Reasoning & Chronology (TR&C)</h4>
            <div class="task-menu-items">
              <button class="task-menu-btn active" data-task="TSO">TSO: Temporal Shot Ordering</button>
              <button class="task-menu-btn" data-task="MPDR">MPDR: Multi-Person Duration Reasoning</button>
              <button class="task-menu-btn" data-task="ASII">ASII: Action Sequence Integrity & Identification</button>
            </div>
          </div>

          <div class="task-family-group">
            <h4>Referring & Tracking (R&T)</h4>
            <div class="task-menu-items">
              <button class="task-menu-btn" data-task="AGBI">AGBI: Appearance-Grounded Behavior Interactions</button>
              <button class="task-menu-btn" data-task="AGAR">AGAR: Appearance-Grounded Attribute Recognition</button>
              <button class="task-menu-btn" data-task="AGLT">AGLT: Appearance-Grounded Localization Trajectory</button>
            </div>
          </div>

          <div class="task-family-group">
            <h4>Global Consistency & Verification (GC&V)</h4>
            <div class="task-menu-items">
              <button class="task-menu-btn" data-task="FAM">FAM: False Action Memory</button>
              <button class="task-menu-btn" data-task="SVA">SVA: Scene Verification Arrangement</button>
              <button class="task-menu-btn" data-task="FOM">FOM: False Object Memory</button>
            </div>
          </div>

          <div class="task-family-group">
            <h4>Multi-Entity Aggregation & Numeracy (MEA&N)</h4>
            <div class="task-menu-items">
              <button class="task-menu-btn" data-task="MEGL">MEGL: Multi-Entities Grounding & Localization</button>
              <button class="task-menu-btn" data-task="AC">AC: Action Counting</button>
              <button class="task-menu-btn" data-task="RLPC">RLPC: Region-Localized People Counting</button>
            </div>
          </div>

        </div>

        <!-- Detail Viewer -->
        <div class="task-viewer">
          <div class="task-header">
            <div class="task-title">
              <span id="viewer-title">TSO: Temporal Shot Ordering</span>
              <span class="task-badge" id="viewer-badge">Temporal Reasoning & Chronology (TR&C)</span>
            </div>
            <p class="task-desc" id="viewer-desc">
              Arrange four shot descriptions into the correct chronological order using content cues alone.
            </p>
            <div class="task-abilities-box">
              <span class="abilities-label">Abilities Tested:</span>
              <span id="viewer-abilities">Understanding event order, high-level scene transitions, chronological
                reconstruction using content cues</span>
            </div>
          </div>

          <div class="task-content">
            <img id="viewer-img" class="task-placeholder-img" src="" alt="Visual example">
            <div class="placeholder-overlay" style="position: absolute; color: #64748b; font-weight: 500;">
              <!-- Image Placeholder -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- MRFS -->
    <section class="results-section" id="mrfs">
      <h2>Evidential Requirement (MRFS)</h2>

      <p class="section-description">
        We quantify how much visual evidence a VideoQA item requires using the
        <em>Minimum Required Frame-Set (MRFS)</em>: the <strong>smallest number of frames that must be integrated for a
          model to answer correctly.</strong>
        Higher MRFS indicates that questions are not solvable from a single salient snapshot and instead require
        integrating temporally separated cues.
      </p>

      <div class="bar-chart-container" style="margin-top: 1rem;">
        <!-- NExT-QA: 2.61 -->
        <div class="bar-row">
          <div class="bar-label">NExT-QA</div>
          <div class="bar-track">
            <div class="bar-fill" style="width: 43.5%;">2.61</div>
          </div>
        </div>
        <!-- MVBench: 3.52 -->
        <div class="bar-row">
          <div class="bar-label">MVBench</div>
          <div class="bar-track">
            <div class="bar-fill" style="width: 58.7%;">3.52</div>
          </div>
        </div>
        <!-- LongVideoBench: 4.07 -->
        <div class="bar-row">
          <div class="bar-label">LongVideoBench</div>
          <div class="bar-track">
            <div class="bar-fill" style="width: 67.8%;">4.07</div>
          </div>
        </div>
        <!-- HERBench: 5.49 -->
        <div class="bar-row">
          <div class="bar-label"><strong>HERBench</strong></div>
          <div class="bar-track">
            <div class="bar-fill highlight" style="width: 91.5%;">5.49</div>
          </div>
        </div>
      </div>

      <p class="section-description" style="margin-top: 1.5rem;">
        <strong>Cross-benchmark comparison.</strong>
        HERBench exhibits the highest evidential requirement (mean MRFS = 5.49), exceeding LongVideoBench (4.07),
        MVBench (3.52), and NExT-QA (2.61).
        Notably, this higher MRFS is achieved despite a shorter average video duration than LongVideoBench, suggesting
        the difficulty is driven by evidential density rather than video length alone.
        This metric-level effect is consistent with the benchmark construction, which explicitly filters out
        items that are solvable with too few frames.
      </p>

      <p class="section-description" style="font-size: 0.95em; opacity: 0.85; margin-top: 0.75rem;">
        <strong>Protocol.</strong>
        Because MRFS is defined with respect to a model <em>f</em>, a question-conditioned frame selector <em>r</em>,
        and a frame budget <em>x</em>,
        we report values under a standardized setup to enable apples-to-apples comparison across benchmarks.
      </p>
    </section>

    <!-- Additional Figures -->
    <section class="comparison-section" id="figures">
      <h2>Benchmark statistics</h2>
      <p class="bench-note">
        The plots below give a quick snapshot of HERBench: where the questions come from, what they talk about, and how
        they’re distributed across tasks.
      </p>

      <ul class="bench-bullets">
        <li><span class="bench-label">Source mix</span> — question count by dataset.</li>
        <li><span class="bench-label">Language</span> — frequent terms in the questions (word cloud).</li>
        <li><span class="bench-label">Coverage</span> — question count across tasks.</li>
      </ul>
      <div class="figure-grid">
        <div class="figure-card">
          <div class="stats-layout">
            <div class="stats-top-row">
              <div class="stats-img-half">
                <img src="assets/images/questions_per_dataset_pie.png" alt="Questions per Dataset" class="stats-img"
                  loading="lazy">
              </div>
              <div class="stats-img-half">
                <img src="assets/images/wordcloud.jpg" alt="Wordcloud" class="stats-img" loading="lazy">
              </div>
            </div>
            <div class="stats-img-full">
              <img src="assets/images/task_bar_chart.png" alt="Task Distribution Bar Chart" class="stats-img"
                loading="lazy">
            </div>
          </div>
        </div>

      </div>
    </section>



    <!-- Download & Evaluate -->
    <section class="method-section" id="download">
      <h2>Download and Evaluate</h2>

      <div class="card-grid">
        <!-- Card 1: Setup -->
        <div class="card">
          <h3>1. Setup</h3>
          <p><strong>Prerequisites:</strong> Python 3.8-3.12, PyTorch 2.0+, and a GPU.</p>
          <div class="code-block" style="margin-top: 1rem;">
            <pre><code># Clone and install
git clone https://github.com/DanBenAmi/HERBench.git
cd HERBench

# Create env
conda env create -f environment.yml
conda activate herbench
pip install -e .</code></pre>
          </div>
        </div>

        <!-- Card 2: Data -->
        <div class="card">
          <h3>2. Get Data</h3>
          <p>Download video features and annotation files.</p>
          <div class="code-block" style="margin-top: 1rem;">
            <pre><code># Via Hugging Face (recommended)
python scripts/download_data.py --source huggingface

# Or via direct download
python scripts/download_data.py --source direct</code></pre>
          </div>
        </div>

        <!-- Card 3: Evaluate -->
        <div class="card" style="grid-column: 1 / -1;">
          <h3>3. Evaluate</h3>
          <p>Run inference and calculate metrics.</p>
          <div class="code-grid" style="margin-top: 1rem;">
            <div class="code-block">
              <h4>Run Inference</h4>
              <pre><code># Qwen2.5-VL (uniform frames)
python evaluation/run_evaluation.py \
    model=qwen25vl frame_selector=uniform

# InternVL3.5 (BLIP frames)
python evaluation/run_evaluation.py \
    model=internvl35 frame_selector=blip</code></pre>
            </div>
            <div class="code-block">
              <h4>Calculate Metrics</h4>
              <pre><code># Accuracy
python evaluation/calculate_accuracy.py \
    --predictions results/predictions.json

# MRFS
python evaluation/calculate_mrfs.py \
    model=qwen25vl frame_selector=blip</code></pre>
            </div>
          </div>
        </div>
      </div>
    </section>




    <!-- Citation -->
    <section class="bibtex-section" id="citation">
      <h2>Citation</h2>
      <pre class="bibtex"><code>@misc{benami2025herbenchbenchmarkmultievidenceintegration,
      title={HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering}, 
      author={Dan Ben-Ami and Gabriele Serussi and Kobi Cohen and Chaim Baskin},
      year={2025},
      eprint={2512.14870},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.14870}, 
}</code></pre>
    </section>

    <!-- Footer -->
    <footer class="page-footer">
      <p>
        (c) 2025 HERBench Team. You are welcome to reuse or adapt this page's source code under the MIT License.
      </p>
      <p>
        Please include a link back to HERBench if you use this template. For questions, open an issue on GitHub or email
        danbenami3@gmail.com, serussigabriele@gmail.com.
      </p>
    </footer>
  </div>

  <script src="assets/js/main.js"></script>
</body>

</html>